{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32d82389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6853717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Load data\n",
    "# ------------------------------\n",
    "train_df = pd.read_csv(\"train_split.csv\")\n",
    "val_df   = pd.read_csv(\"val_split.csv\")\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join(c for c in text if c.isalnum() or c.isspace())\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
    "val_df['clean_text']   = val_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "806445f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Aspect multi-label encoding\n",
    "# ------------------------------\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_aspect = mlb.fit_transform(train_df.groupby('id')['aspectCategory'].apply(list))\n",
    "y_val_aspect   = mlb.transform(val_df.groupby('id')['aspectCategory'].apply(list))\n",
    "\n",
    "# Corresponding texts per id\n",
    "X_train_aspect = train_df.groupby('id')['clean_text'].first().tolist()\n",
    "X_val_aspect   = val_df.groupby('id')['clean_text'].first().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f61079fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Sentiment multi-class encoding\n",
    "# ------------------------------\n",
    "le_sent = LabelEncoder()\n",
    "y_train_sent = le_sent.fit_transform(train_df['polarity'])\n",
    "y_val_sent   = le_sent.transform(val_df['polarity'])\n",
    "X_train_sent = train_df['aspectCategory'] + \" : \" + train_df['clean_text']\n",
    "X_val_sent   = val_df['aspectCategory'] + \" : \" + val_df['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d74c27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3724\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Tokenization (simple word2index)\n",
    "# ------------------------------\n",
    "# Convert all relevant text to list first\n",
    "X_train_aspect_list = list(X_train_aspect)  # already grouped by id\n",
    "X_train_sent_list   = list(X_train_sent)    # full row-wise\n",
    "\n",
    "# Build vocabulary from training texts only\n",
    "word2idx = {\"<PAD>\": 0}  # fixed mapping\n",
    "\n",
    "for text in X_train_aspect_list + X_train_sent_list:\n",
    "    for word in text.split():\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize(text):\n",
    "    # Map unknown words to <PAD> (index 0)\n",
    "    return [word2idx.get(word, 0) for word in text.split()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2624b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_len=50):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = tokenize(self.texts[idx])\n",
    "        tokens = tokens[:self.max_len] + [0]*(self.max_len - len(tokens))  # pad\n",
    "        x = torch.tensor(tokens, dtype=torch.long)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float if self.labels.ndim>1 else torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Max sequence lengths\n",
    "max_len_aspect = max(len(t.split()) for t in X_train_aspect)\n",
    "max_len_sent   = max(len(t.split()) for t in X_train_sent)\n",
    "\n",
    "# Datasets and loaders\n",
    "train_aspect_ds = ABSADataset(X_train_aspect, y_train_aspect, max_len_aspect)\n",
    "val_aspect_ds   = ABSADataset(X_val_aspect, y_val_aspect, max_len_aspect)\n",
    "train_sent_ds   = ABSADataset(X_train_sent, y_train_sent, max_len_sent)\n",
    "val_sent_ds     = ABSADataset(X_val_sent, y_val_sent, max_len_sent)\n",
    "\n",
    "batch_size = 16\n",
    "train_aspect_loader = DataLoader(train_aspect_ds, batch_size=batch_size, shuffle=True)\n",
    "val_aspect_loader   = DataLoader(val_aspect_ds, batch_size=batch_size)\n",
    "train_sent_loader   = DataLoader(train_sent_ds, batch_size=batch_size, shuffle=True)\n",
    "val_sent_loader     = DataLoader(val_sent_ds, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4d58dc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, task='multi-label'):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.task = task\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)           # (batch, seq_len, embed_dim)\n",
    "        avg_emb = emb.mean(dim=1)         # average over words\n",
    "        out = F.relu(self.fc1(avg_emb))\n",
    "        out = self.dropout(out)\n",
    "        if self.task == 'multi-label':\n",
    "            return torch.sigmoid(self.fc2(out))\n",
    "        else:\n",
    "            return self.fc2(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d94496b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def eval_model(model, loader, criterion, device, task='multi-label'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            all_preds.append(y_pred.cpu())\n",
    "            all_labels.append(y.cpu())\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    if task == 'multi-label':\n",
    "        all_preds_bin = (all_preds > 0.5).int()\n",
    "        return total_loss / len(loader.dataset), all_preds_bin, all_labels\n",
    "    else:\n",
    "        all_preds_class = torch.argmax(all_preds, dim=1)\n",
    "        return total_loss / len(loader.dataset), all_preds_class, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fd02479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.6929, Val Loss=0.6852\n",
      "Epoch 2: Train Loss=0.6803, Val Loss=0.6706\n",
      "Epoch 3: Train Loss=0.6659, Val Loss=0.6530\n",
      "Epoch 4: Train Loss=0.6483, Val Loss=0.6319\n",
      "Epoch 5: Train Loss=0.6279, Val Loss=0.6082\n",
      "Epoch 6: Train Loss=0.6071, Val Loss=0.5849\n",
      "Epoch 7: Train Loss=0.5873, Val Loss=0.5644\n",
      "Epoch 8: Train Loss=0.5710, Val Loss=0.5480\n",
      "Epoch 9: Train Loss=0.5577, Val Loss=0.5356\n",
      "Epoch 10: Train Loss=0.5478, Val Loss=0.5265\n",
      "Epoch 11: Train Loss=0.5396, Val Loss=0.5196\n",
      "Epoch 12: Train Loss=0.5335, Val Loss=0.5142\n",
      "Epoch 13: Train Loss=0.5285, Val Loss=0.5098\n",
      "Epoch 14: Train Loss=0.5236, Val Loss=0.5060\n",
      "Epoch 15: Train Loss=0.5187, Val Loss=0.5027\n",
      "Epoch 16: Train Loss=0.5139, Val Loss=0.4994\n",
      "Epoch 17: Train Loss=0.5102, Val Loss=0.4964\n",
      "Epoch 18: Train Loss=0.5057, Val Loss=0.4935\n",
      "Epoch 19: Train Loss=0.5026, Val Loss=0.4908\n",
      "Epoch 20: Train Loss=0.4979, Val Loss=0.4882\n",
      "Epoch 21: Train Loss=0.4952, Val Loss=0.4856\n",
      "Epoch 22: Train Loss=0.4902, Val Loss=0.4831\n",
      "Epoch 23: Train Loss=0.4861, Val Loss=0.4807\n",
      "Epoch 24: Train Loss=0.4827, Val Loss=0.4784\n",
      "Epoch 25: Train Loss=0.4785, Val Loss=0.4762\n",
      "Epoch 26: Train Loss=0.4748, Val Loss=0.4741\n",
      "Epoch 27: Train Loss=0.4711, Val Loss=0.4719\n",
      "Epoch 28: Train Loss=0.4674, Val Loss=0.4699\n",
      "Epoch 29: Train Loss=0.4644, Val Loss=0.4681\n",
      "Epoch 30: Train Loss=0.4609, Val Loss=0.4662\n",
      "Epoch 31: Train Loss=0.4575, Val Loss=0.4645\n",
      "Epoch 32: Train Loss=0.4543, Val Loss=0.4628\n",
      "Epoch 33: Train Loss=0.4511, Val Loss=0.4612\n",
      "Epoch 34: Train Loss=0.4482, Val Loss=0.4596\n",
      "Epoch 35: Train Loss=0.4460, Val Loss=0.4580\n",
      "Epoch 36: Train Loss=0.4419, Val Loss=0.4565\n",
      "Epoch 37: Train Loss=0.4393, Val Loss=0.4550\n",
      "Epoch 38: Train Loss=0.4366, Val Loss=0.4537\n",
      "Epoch 39: Train Loss=0.4344, Val Loss=0.4524\n",
      "Epoch 40: Train Loss=0.4322, Val Loss=0.4511\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Aspect classifier\n",
    "dan_aspect = DAN(vocab_size, embed_dim=300, hidden_dim=256, output_dim=y_train_aspect.shape[1], task='multi-label').to(device)\n",
    "optimizer_aspect = torch.optim.Adam(dan_aspect.parameters(), lr=3e-5)\n",
    "criterion_aspect = nn.BCELoss()\n",
    "\n",
    "for epoch in range(40):\n",
    "    train_loss = train_model(dan_aspect, train_aspect_loader, optimizer_aspect, criterion_aspect, device)\n",
    "    val_loss, val_preds_aspect, val_labels_aspect = eval_model(dan_aspect, val_aspect_loader, criterion_aspect, device)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "98c699c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.3176, Val Loss=1.2156\n",
      "Epoch 2: Train Loss=1.1361, Val Loss=1.0786\n",
      "Epoch 3: Train Loss=1.0603, Val Loss=1.0475\n",
      "Epoch 4: Train Loss=1.0369, Val Loss=1.0297\n",
      "Epoch 5: Train Loss=1.0132, Val Loss=1.0128\n",
      "Epoch 6: Train Loss=0.9931, Val Loss=0.9956\n",
      "Epoch 7: Train Loss=0.9719, Val Loss=0.9795\n",
      "Epoch 8: Train Loss=0.9465, Val Loss=0.9618\n",
      "Epoch 9: Train Loss=0.9276, Val Loss=0.9463\n",
      "Epoch 10: Train Loss=0.9018, Val Loss=0.9316\n",
      "Epoch 11: Train Loss=0.8811, Val Loss=0.9189\n",
      "Epoch 12: Train Loss=0.8634, Val Loss=0.9059\n",
      "Epoch 13: Train Loss=0.8430, Val Loss=0.8950\n",
      "Epoch 14: Train Loss=0.8252, Val Loss=0.8848\n",
      "Epoch 15: Train Loss=0.8012, Val Loss=0.8763\n",
      "Epoch 16: Train Loss=0.7922, Val Loss=0.8679\n",
      "Epoch 17: Train Loss=0.7692, Val Loss=0.8602\n",
      "Epoch 18: Train Loss=0.7515, Val Loss=0.8537\n",
      "Epoch 19: Train Loss=0.7359, Val Loss=0.8481\n",
      "Epoch 20: Train Loss=0.7170, Val Loss=0.8432\n",
      "Epoch 21: Train Loss=0.7029, Val Loss=0.8383\n",
      "Epoch 22: Train Loss=0.6875, Val Loss=0.8339\n",
      "Epoch 23: Train Loss=0.6721, Val Loss=0.8310\n",
      "Epoch 24: Train Loss=0.6604, Val Loss=0.8282\n",
      "Epoch 25: Train Loss=0.6470, Val Loss=0.8248\n",
      "Epoch 26: Train Loss=0.6297, Val Loss=0.8245\n",
      "Epoch 27: Train Loss=0.6221, Val Loss=0.8210\n",
      "Epoch 28: Train Loss=0.6049, Val Loss=0.8200\n",
      "Epoch 29: Train Loss=0.5962, Val Loss=0.8214\n",
      "Epoch 30: Train Loss=0.5824, Val Loss=0.8182\n"
     ]
    }
   ],
   "source": [
    "# Sentiment classifier\n",
    "dan_sent = DAN(vocab_size, embed_dim=300, hidden_dim=256, output_dim=len(le_sent.classes_), task='multi-class').to(device)\n",
    "optimizer_sent = torch.optim.Adam(dan_sent.parameters(), lr=1e-4)\n",
    "criterion_sent = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(30):\n",
    "    train_loss = train_model(dan_sent, train_sent_loader, optimizer_sent, criterion_sent, device)\n",
    "    val_loss, val_preds_sent, val_labels_sent = eval_model(dan_sent, val_sent_loader, criterion_sent, device, task='multi-class')\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rows = []\n",
    "aspect_labels = mlb.classes_\n",
    "\n",
    "dan_aspect.eval()\n",
    "dan_sent.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, text in enumerate(X_val_aspect):\n",
    "        tokens = tokenize(text)\n",
    "        tokens = tokens[:max_len_aspect] + [0]*(max_len_aspect - len(tokens))\n",
    "        x_tensor = torch.tensor([tokens], dtype=torch.long).to(device)\n",
    "        aspect_probs = dan_aspect(x_tensor)\n",
    "        aspect_pred = (aspect_probs > 0.5)[0].cpu().numpy()\n",
    "        for j, val in enumerate(aspect_pred):\n",
    "            if val == 1:\n",
    "                aspect_name = aspect_labels[j]\n",
    "                input_text = aspect_name + \" : \" + text\n",
    "                tokens_sent = tokenize(input_text)\n",
    "                tokens_sent = tokens_sent[:max_len_sent] + [0]*(max_len_sent - len(tokens_sent))\n",
    "                x_sent = torch.tensor([tokens_sent], dtype=torch.long).to(device)\n",
    "                pred_sent = torch.argmax(dan_sent(x_sent), dim=1).item()\n",
    "                pred_sent_label = le_sent.inverse_transform([pred_sent])[0]\n",
    "                pred_rows.append({\"id\": val_df.iloc[i][\"id\"], \"aspectCategory\": aspect_name, \"polarity\": pred_sent_label})\n",
    "\n",
    "val_pred = pd.DataFrame(pred_rows)\n",
    "val_pred.to_csv(\"val_pred_DAN_PyTorch.csv\", index=False)\n",
    "val_df[['id', 'aspectCategory', 'polarity']].to_csv(\"val_truth.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
